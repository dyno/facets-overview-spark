// Copyright Â© 2018 GoPro, Inc. All Rights Reserved
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
// ==============================================================================
//

package features.stats.spark

import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{ArrayType, StructField, StructType}
import org.apache.spark.sql.{functions, Column, DataFrame}

/**
 * Created by chesterchen on 6/2/18.
 */
object DataFrameUtils {

  def isNestedArrayType(field: StructField): Boolean =
    field.dataType match {
      // example: tf.train.SequenceExample
      case ArrayType(ArrayType(_, _), _) => true
      case _                             => false
    }

  /**
   * Flatten the names of all attributes of nested structures into one "flat" structure where the name of the
   * attribute is the concatenation of all names of the parent structures down to the actual name of the attribute
   * being flattened with periods being inserted between the names.
   *
   * @param schema a StructType describing the schema or structure of a data frame
   * @param prefix a prefix to prepend to all flattened column names
   *
   * @return an array of columns with flattened names
   */
  private[spark] def flattenSchemaNames(schema: StructType, prefix: String = null): Array[Column] =
    schema.fields.flatMap { f =>
      val colName = if (prefix == null) f.name else s"$prefix.${f.name}"

      f.dataType match {
        case st: StructType => flattenSchemaNames(st, colName)
        case _              => Array(col(colName))
      }
    }

  /**
   * Flatten the nested Spark data frame into one "flat" structure.
   * Also, explode the arrayType columns into multiple rows.
   */
  def flattenDataFrame(df: DataFrame, recursive: Boolean = true): DataFrame = {
    var flattenedDf = df.select(flattenSchemaNames(df.schema).map(col => col.alias(sanitizedName(col))): _*)

    flattenedDf.schema.fields.foreach { x =>
      x.dataType match {
        case st: ArrayType =>
          // only explode the array when it is not empty otherwise it will lose the entry
          val explodedCol = functions.explode(
            when(size(col(x.name)) > 0, col(x.name))
              .otherwise(array(lit(null).cast(st.elementType)))
          )

          val df = flattenedDf.withColumn(x.name, explodedCol)
          flattenedDf = if (recursive) flattenDataFrame(df, recursive) else df

        case _ => //ignore
      }
    }

    flattenedDf
  }

  private def sanitizedName(col: Column) =
    col.toString().replace(".", "_").replace("-", "_").replace("__", "_").replaceFirst("^_", "").toLowerCase

  def sanitizedNames(cols: Array[String]): Array[String] =
    cols.map(col => col.replace(".", "_").replace("-", "_").replace("__", "_").replaceFirst("^_", "").toLowerCase)
}
